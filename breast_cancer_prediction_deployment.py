# -*- coding: utf-8 -*-
"""breast-cancer-prediction : deployment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Phr-l9SwtLmQyAWFRbPE-Cp00X_UQQSa

# Breast Cancer Data Analysis and Predictions

###  Breast Cancer

# 1. Exploratory Data Analysis

## 1.1 Understanding the data
"""

# Commented out IPython magic to ensure Python compatibility.
# importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from google.colab import drive
from google.colab import files

# Prompt user to upload the file
uploaded = files.upload()

# Assume the file is named 'data.csv'
file_name = 'data.csv'

# Read the uploaded CSV file into a pandas DataFrame
df = pd.read_csv(file_name)

# Display the first few rows of the DataFrame
print(df.head())

# displaying first five rows
df.head()

# shape of the dataframe
df.shape

# concise summary of dataframe
df.info()

# column names
df.columns

# checking for null values
df.isnull().sum()

"""The whole column 'Unamed: 32' has NaN values."""

# dropping 'Unnamed: 32' column.
df.drop("Unnamed: 32", axis=1, inplace=True)

# dropping id column
df.drop('id',axis=1, inplace=True)

# descriptive statistics of data
df.describe()

"""## 1.2. Data Visualizations"""

# countplot
plt.figure(figsize = (8,7))
sns.countplot(x="diagnosis", data=df, palette='magma')

"""From the heatmap, we can observe from the heatmaps that there are many negative correlations in this dataset."""

df.columns

"""The mean, standard error and "worst" or largest (mean of the three
largest values) of these features were computed for each image,
resulting in 30 features. For instance, field 3 is Mean Radius, field
13 is Radius SE, field 23 is Worst Radius.

# 2. Data Preprocessing and Building Models

## 2.1 Data Preprocessing
"""

# counts of unique rows in the 'diagnosis' column
df['diagnosis'].value_counts()

"""## 2.2 Splitting the data into train and test"""

from sklearn.model_selection import train_test_split

# splitting data
X_train, X_test, y_train, y_test = train_test_split(
                df.drop('diagnosis', axis=1),
                df['diagnosis'],
                test_size=0.2,
                random_state=42)

print("Shape of training set:", X_train.shape)
print("Shape of test set:", X_test.shape)

"""RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import pandas as pd

# Create and train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
rf_predictions = rf_model.predict(X_test)

# Evaluate the model
print("Random Forest Model Results:")
print("Confusion Matrix:")
print(confusion_matrix(y_test, rf_predictions))
print("\nClassification Report:")
print(classification_report(y_test, rf_predictions))

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
import numpy as np

# Convert X_train from NumPy array to Pandas DataFrame
X_train_df = pd.DataFrame(X_train)

# Now use X_train_df instead of X_train
feature_importance = pd.DataFrame({
    'feature': X_train_df.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Convert X_train from NumPy array to Pandas DataFrame
X_train_df = pd.DataFrame(X_train)

# Use X_train_df instead of X_train
feature_importance = pd.DataFrame({
    'feature': X_train_df.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

# Feature importance plot
plt.figure(figsize=(20, 8))
sns.barplot(x='importance', y='feature', data=feature_importance.head(10))
plt.title('Top 15 Most Important Features in Random Forest Model')
plt.tight_layout()
plt.show()

# Define the feature names based on your dataset
feature_names = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean',
                 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean',
                 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se',
                 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',
                 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst',
                 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst',
                 'symmetry_worst', 'fractal_dimension_worst']

# Calculate feature importance
feature_importance = pd.DataFrame({
    'feature': feature_names,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

# Plot
plt.figure(figsize=(14, 10))  # Increased figure size for better readability
ax = sns.barplot(x='importance', y='feature', data=feature_importance.head(15))

# Customize the plot
plt.title('Top 15 Most Important Features in Random Forest Model', fontsize=16)
plt.xlabel('Importance', fontsize=12)
plt.ylabel('Features', fontsize=12)

# Format x-axis to 3 decimal places
ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:.3f}'))

# Adjust layout and display
plt.tight_layout()
plt.show()

# Define the feature names based on your dataset
feature_names = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean',
                 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean',
                 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se',
                 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',
                 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst',
                 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst',
                 'symmetry_worst', 'fractal_dimension_worst']

# Calculate feature importance
feature_importance = pd.DataFrame({
    'feature': feature_names,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

# Plot
plt.figure(figsize=(14, 10))  # Increased figure size for better readability

# Use a color palette
color_palette = sns.color_palette("husl", 15)  # Generate 15 distinct colors

ax = sns.barplot(x='importance', y='feature', data=feature_importance.head(15), palette=color_palette)

# Customize the plot
plt.title('Top 15 Most Important Features in Random Forest Model', fontsize=16)
plt.xlabel('Importance', fontsize=12)
plt.ylabel('Features', fontsize=12)

# Format x-axis to 3 decimal places
ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:.3f}'))

# Adjust layout and display
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Define the feature names based on your dataset
feature_names = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean',
                 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean',
                 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se',
                 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',
                 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst',
                 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst',
                 'symmetry_worst', 'fractal_dimension_worst']

# Assuming rf_model is your trained Random Forest model
# Calculate feature importance
feature_importance = pd.DataFrame({
    'feature': feature_names,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

# Create a dictionary for the word cloud
word_freq = {row['feature']: row['importance'] for index, row in feature_importance.iterrows()}

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

# Display the generated word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Turn off axis numbers and ticks
plt.title('Word cloud of features in Breast Cancer Dataset')
plt.show()

# Confusion matrix
y_pred = rf_model.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Convert X_train from NumPy array to pandas DataFrame
X_train_df = pd.DataFrame(X_train, columns=['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean',
                                            'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean',
                                            'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se',
                                            'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',
                                            'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst',
                                            'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst',
                                            'symmetry_worst', 'fractal_dimension_worst'])

# Correlation heatmap
plt.figure(figsize=(20, 16))
sns.heatmap(X_train_df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap of Breast Cancer Features')
plt.tight_layout()
plt.show()

"""### MLP"""

from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

# Prepare the target variable
y_train = y_train.map({'M': 1, 'B': 0})  # Map 'M' to 1 and 'B' to 0 for binary classification
y_test = y_test.map({'M': 1, 'B': 0})    # Ensure y_test is also mapped

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build the MLP model
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dropout(0.3),  # Dropout layer for regularization
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dropout(0.3),  # Dropout layer for regularization
    keras.layers.Dense(16, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')  # Sigmoid activation for binary classification
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)
print(f"Test accuracy: {test_accuracy:.4f}")

# Make predictions on the test set
predictions = model.predict(X_test_scaled)
predictions = (predictions > 0.5).astype(int)

# Print classification report
print(classification_report(y_test, predictions))

!pip install streamlit

model.save('breast_cancer_model.h5')

model = keras.models.load_model('breast_cancer_model.h5')

model = keras.models.load_model('/path/to/breast_cancer_model.h5')

import streamlit as st
import pandas as pd
from sklearn.preprocessing import StandardScaler
from tensorflow import keras

# Load the model
model = keras.models.load_model('breast_cancer_model.h5')

# Load the scaler
scaler = StandardScaler()

st.title('Breast Cancer Prediction')

# Create input fields for features
feature_names = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean',
                 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean',
                 'fractal_dimension_mean']

input_data = {}
for feature in feature_names:
    input_data[feature] = st.number_input(f'Enter {feature}', value=0.0, format="%.6f")

if st.button('Predict'):
    # Prepare input data
    input_df = pd.DataFrame([input_data])

    # Scale the input data
    input_scaled = scaler.transform(input_df)

    # Make prediction
    prediction = model.predict(input_scaled)[0][0]

    if prediction >= 0.5:
        st.error(f'The prediction indicates a high likelihood of malignant breast cancer. Probability: {prediction:.2%}')
    else:
        st.success(f'The prediction indicates a low likelihood of malignant breast cancer. Probability: {prediction:.2%}')

st.write('**Note:** This tool is for educational purposes only. Always consult with a medical professional for accurate diagnosis.')

import pickle
from tensorflow import keras

import pickle

# Save the model to a pickle file
with open('breast_cancer_model.pkl', 'wb') as file:
    pickle.dump(model, file)

print("Model saved as breast_cancer_model.pkl")

from google.colab import files

# Download the pickle file
files.download('breast_cancer_model.pkl')